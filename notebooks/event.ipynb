{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = os.path.abspath('..')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "from src.loader import NewsDataLoader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = \"../data\"\n",
    "    loader = NewsDataLoader(data_directory)\n",
    "\n",
    "    merge_df = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words and apply stemming\n",
    "    filtered_tokens = [ps.stem(word) for word in tokens if word not in stop_words]\n",
    "    # Join the filtered tokens back into text\n",
    "    preprocessed_text = ' '.join(filtered_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "merge_df['preprocessed_content'] = merge_df['content'].apply(preprocess_text)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(merge_df['preprocessed_content'])\n",
    "\n",
    "#  Clustering\n",
    "num_clusters = 10  # Choose an appropriate number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "\n",
    "merge_df['cluster'] = kmeans.labels_\n",
    "\n",
    "# Count the number of events covered in the data\n",
    "num_events = merge_df['cluster'].nunique()\n",
    "print(\"Number of events:\", num_events)\n",
    "\n",
    "# Determine which news sites report events the earliest\n",
    "earliest_reporting = merge_df.groupby('source_name')['published_at'].min()\n",
    "earliest_reporting = earliest_reporting.sort_values()\n",
    "print(\"News sites reporting events earliest:\\n\", earliest_reporting)\n",
    "\n",
    "event_counts = merge_df['cluster'].value_counts()\n",
    "print(\"Event reporting frequency:\\n\", event_counts)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(event_counts.index, event_counts.values, color='skyblue')\n",
    "plt.xlabel('Event Cluster')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Event Reporting Frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
